
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif, f_classif, SelectFromModel, VarianceThreshold, RFE
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import MinMaxScaler

def _convert_numpy_types(obj):
    """ØªØ­ÙˆÙŠÙ„ Ø£Ù†ÙˆØ§Ø¹ NumPy Ø¥Ù„Ù‰ Ø£Ù†ÙˆØ§Ø¹ Python Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ JSON"""
    if isinstance(obj, (np.integer, np.int32, np.int64)):
        return int(obj)
    elif isinstance(obj, (np.floating, np.float32, np.float64)):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, pd.Series):
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø³Ù„Ø³Ù„Ø© Ø¥Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ù…Ø¹ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù‚ÙŠÙ…
        return obj.astype(float).tolist()
    elif isinstance(obj, dict):
        return {k: _convert_numpy_types(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [_convert_numpy_types(item) for item in obj]
    elif pd.isna(obj):
        return None
    else:
        return obj

def _safe_y_int(y):
    """ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…ØªØºÙŠØ± Ø§Ù„Ù‡Ø¯Ù Ø¥Ù„Ù‰ Ø£Ø¹Ø¯Ø§Ø¯ ØµØ­ÙŠØ­Ø© Ø¨Ø´ÙƒÙ„ Ø¢Ù…Ù†"""
    try:
        return y.astype(int)
    except Exception:
        return pd.to_numeric(y, errors='coerce').fillna(0).astype(int)

def _safe_cv_score(model, X, y, cv=5):
    """Ø­Ø³Ø§Ø¨ Ø¯Ù‚Ø© Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ù…ØªÙ‚Ø§Ø·Ø¹ Ø¨Ø´ÙƒÙ„ Ø¢Ù…Ù† Ù…Ø¹ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØªÙŠØ¬Ø©"""
    try:
        score = np.mean(cross_val_score(model, X, y, cv=cv, scoring='accuracy'))
        return _convert_numpy_types(score)
    except Exception:
        return None

def embedding_rf(df, target_name, top_k=None):
    """Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ù…ÙŠØ²Ø§Øª ÙÙŠ Random Forest"""
    X = df.drop(columns=[target_name])
    y = _safe_y_int(df[target_name])
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    try:
        model.fit(X, y)
        importances = pd.Series(model.feature_importances_, index=X.columns).sort_values(ascending=False)
    except Exception:
        importances = pd.Series(0, index=X.columns)

    # ØªØ­Ø¯ÙŠØ¯ Ø¹Ø¯Ø¯ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…Ø®ØªØ§Ø±Ø©
    if top_k is None:
        top_k = max(1, X.shape[1] // 4)
    selected = importances.iloc[:top_k].index.tolist()
    cv_score = _safe_cv_score(model, X[selected], y)
    
    result = {
        'method': 'embedding_rf', 
        'selected_features': selected, 
        'meta': {'importances': _convert_numpy_types(importances.to_dict())}, 
        'cv_score': cv_score
    }
    
    return _convert_numpy_types(result)

def filter_chi2(df, target_name, k=10):
    """Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ø®ØªØ¨Ø§Ø± Chi-square"""
    X = df.drop(columns=[target_name]).copy()
    y = _safe_y_int(df[target_name])
    
    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù‚ÙŠÙ… Ù„ØªÙƒÙˆÙ† ØºÙŠØ± Ø³Ø§Ù„Ø¨Ø© (Ù…Ø·Ù„ÙˆØ¨ Ù„Ù€ Chi-square)
    try:
        Xm = X - X.min().min()
    except Exception:
        Xm = X.copy()
    
    selector = SelectKBest(chi2, k=min(k, X.shape[1]))
    try:
        selector.fit(Xm.abs(), y)
        mask = selector.get_support()
        selected = X.columns[mask].tolist()
        scores = dict(zip(X.columns, selector.scores_))
    except Exception:
        selected, scores = [], {}
    
    # ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    model = LogisticRegression(max_iter=1000)
    cv_score = None
    if selected:
        cv_score = _safe_cv_score(model, X[selected], y)
    
    result = {
        'method': 'filter_chi2', 
        'selected_features': selected, 
        'meta': {'scores': _convert_numpy_types(scores)}, 
        'cv_score': cv_score
    }
    
    return _convert_numpy_types(result)

def mutual_info(df, target_name, k=10):
    """Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØªØ¨Ø§Ø¯Ù„Ø©"""
    X = df.drop(columns=[target_name])
    y = _safe_y_int(df[target_name])
    try:
        selector = SelectKBest(mutual_info_classif, k=min(k, X.shape[1]))
        selector.fit(X, y)
        mask = selector.get_support()
        selected = X.columns[mask].tolist()
        scores = dict(zip(X.columns, selector.scores_))
    except Exception:
        selected, scores = [], {}
    
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    cv_score = _safe_cv_score(model, X[selected], y) if selected else None
    
    result = {
        'method': 'mutual_info', 
        'selected_features': selected, 
        'meta': {'scores': _convert_numpy_types(scores)}, 
        'cv_score': cv_score
    }
    
    return _convert_numpy_types(result)

def f_classif_filter(df, target_name, k=10):
    """Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ø®ØªØ¨Ø§Ø± F (ANOVA)"""
    X = df.drop(columns=[target_name])
    y = _safe_y_int(df[target_name])
    try:
        selector = SelectKBest(f_classif, k=min(k, X.shape[1]))
        selector.fit(X, y)
        mask = selector.get_support()
        selected = X.columns[mask].tolist()
        scores = dict(zip(X.columns, selector.scores_))
    except Exception:
        selected, scores = [], {}
    
    model = LogisticRegression(max_iter=1000)
    cv_score = _safe_cv_score(model, X[selected], y) if selected else None
    
    result = {
        'method': 'f_classif', 
        'selected_features': selected, 
        'meta': {'scores': _convert_numpy_types(scores)}, 
        'cv_score': cv_score
    }
    
    return _convert_numpy_types(result)

def l1_logistic(df, target_name, C=0.1):
    """Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ù„ÙˆØ¬Ø³ØªÙŠ Ù…Ø¹ Ø¹Ù‚ÙˆØ¨Ø© L1"""
    X = df.drop(columns=[target_name])
    y = _safe_y_int(df[target_name])
    
    print(f"ğŸ” ØªØ´ØºÙŠÙ„ L1 Logistic Ù…Ø¹ C={C}")
    print(f"ğŸ“Š Ø´ÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {X.shape}")
    
    try:
        # ØªØ­Ø¬ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        scaler = MinMaxScaler()
        Xs = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)
        
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… solver Ù…Ù†Ø§Ø³Ø¨ Ù„Ù€ L1
        model = LogisticRegression(
            penalty='l1', 
            solver='liblinear',  # liblinear Ø£ÙØ¶Ù„ Ù„Ù€ L1
            C=C, 
            max_iter=1000,
            random_state=42
        )
        
        selector = SelectFromModel(model, threshold='mean')
        selector.fit(Xs, y)
        mask = selector.get_support()
        selected = X.columns[mask].tolist()
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        if hasattr(selector.estimator_, 'coef_'):
            importances = np.abs(selector.estimator_.coef_.ravel())
            importances_dict = dict(zip(X.columns, importances))
        else:
            importances_dict = {}
            
        print(f"âœ… L1 Logistic: ØªÙ… Ø§Ø®ØªÙŠØ§Ø± {len(selected)} Ù…ÙŠØ²Ø©")
        print(f"ğŸ“‹ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…Ø®ØªØ§Ø±Ø©: {selected}")
        
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ L1 Logistic: {str(e)}")
        selected, importances_dict = [], {}
    
    # ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    cv_model = LogisticRegression(max_iter=1000, random_state=42)
    cv_score = _safe_cv_score(cv_model, X[selected], y) if selected else 0.0
    
    # Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙ… Ø§Ø®ØªÙŠØ§Ø± Ø£ÙŠ Ù…ÙŠØ²Ø§ØªØŒ Ù†Ø³ØªØ®Ø¯Ù… Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ù„Ù„ØªÙ‚ÙŠÙŠÙ…
    if not selected:
        cv_score = _safe_cv_score(cv_model, X, y) or 0.0
        print("âš ï¸  Ù„Ù… ÙŠØªÙ… Ø§Ø®ØªÙŠØ§Ø± Ø£ÙŠ Ù…ÙŠØ²Ø§Øª ÙÙŠ L1 LogisticØŒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ù„Ù„ØªÙ‚ÙŠÙŠÙ…")
    
    result = {
        'method': 'l1_logistic', 
        'selected_features': selected, 
        'meta': {'importances': _convert_numpy_types(importances_dict)}, 
        'cv_score': cv_score
    }
    
    print(f"ğŸ¯ Ù†ØªÙŠØ¬Ø© L1 Logistic: {cv_score}")
    return _convert_numpy_types(result)

def rfe_rf(df, target_name, n_features_to_select=None):
    """Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø¹ÙˆØ¯ÙŠØ© Ù„Ù„Ù…ÙŠØ²Ø§Øª Ù…Ø¹ Random Forest"""
    X = df.drop(columns=[target_name])
    y = _safe_y_int(df[target_name])
    n_total = X.shape[1]
    
    if n_features_to_select is None:
        n_features_to_select = max(1, n_total // 4)
    
    try:
        estimator = RandomForestClassifier(n_estimators=100, random_state=42)
        selector = RFE(estimator=estimator, n_features_to_select=n_features_to_select, step=1)
        selector.fit(X, y)
        mask = selector.get_support()
        selected = X.columns[mask].tolist()
        ranking = dict(zip(X.columns, selector.ranking_))
    except Exception:
        selected, ranking = [], {}
    
    cv_score = _safe_cv_score(estimator, X[selected], y) if selected else None
    
    result = {
        'method': 'rfe_rf', 
        'selected_features': selected, 
        'meta': {'ranking': _convert_numpy_types(ranking)}, 
        'cv_score': cv_score
    }
    
    return _convert_numpy_types(result)

def variance_threshold(df, target_name, threshold=0.0):
    """Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø°Ø§Øª Ø§Ù„ØªØ¨Ø§ÙŠÙ† Ø§Ù„Ù…Ù†Ø®ÙØ¶"""
    X = df.drop(columns=[target_name])
    y = _safe_y_int(df[target_name])
    try:
        selector = VarianceThreshold(threshold=threshold)
        Xs = selector.fit_transform(X)
        mask = selector.get_support()
        selected = X.columns[mask].tolist()
    except Exception:
        selected = []
    
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    cv_score = _safe_cv_score(model, X[selected], y) if selected else None
    
    result = {
        'method': 'variance_threshold', 
        'selected_features': selected, 
        'meta': {'threshold': _convert_numpy_types(threshold)}, 
        'cv_score': cv_score
    }
    
    return _convert_numpy_types(result)